import os
import argparse
import torch
import fitz  # PyMuPDF
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


# =========================
# Argument parsing
# =========================
parser = argparse.ArgumentParser()
parser.add_argument("--device_id", type=int, default=0)
args = parser.parse_args()


# =========================
# Path configuration
# =========================
pdf_path = "/ssd0/data/file_name"

output_dir = "outputs"
literature_text_dir = os.path.join(
    output_dir, "literature_text_file_name"
)
extracted_text_path = os.path.join(
    output_dir, "literature file name"
)

os.makedirs(literature_text_dir, exist_ok=True)
os.makedirs(extracted_text_path, exist_ok=True)


# =========================
# PDF processing (optional)
# =========================
process_pdf = False

if process_pdf:
    print("Processing PDF files...")
    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith(".pdf")]
    print(f"Total PDFs: {len(pdf_files)}")

    for pdf_file in tqdm(pdf_files):
        pdf_file_path = os.path.join(pdf_path, pdf_file)
        pdf_name = os.path.splitext(pdf_file)[0]
        pdf_output_dir = os.path.join(literature_text_dir, pdf_name)
        os.makedirs(pdf_output_dir, exist_ok=True)

        try:
            with fitz.open(pdf_file_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)

                    # Extract text
                    page_text = page.get_text()
                    text_output_path = os.path.join(
                        pdf_output_dir, f"page_{page_num + 1}.txt"
                    )
                    with open(text_output_path, "w", encoding="utf-8") as f:
                        f.write(page_text)

        except Exception:
            continue


# =========================
# Model loading
# =========================
num_gpus = 8
model_name = "/ssd0/Qwen/Qwen2.5-32B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)


# =========================
# Text processing
# =========================
print("Processing text files...")

all_pdf_dirs = sorted(os.listdir(literature_text_dir))
num_pdfs = len(all_pdf_dirs)
split_len = num_pdfs // num_gpus

start_idx = args.device_id * split_len
end_idx = (args.device_id + 1) * split_len
selected_pdfs = all_pdf_dirs[start_idx:end_idx]

print(f"Processing PDF indices from {start_idx} to {end_idx}")

for pdf_dir in tqdm(selected_pdfs):
    pdf_dir_path = os.path.join(literature_text_dir, pdf_dir)
    page_files = sorted(
        [f for f in os.listdir(pdf_dir_path) if f.endswith(".txt")]
    )

    output_file_path = os.path.join(
        extracted_text_path, f"{pdf_dir}.txt"
    )

    with open(output_file_path, "w", encoding="utf-8") as out_f:
        for page_index, page_file in enumerate(page_files):
            page_file_path = os.path.join(pdf_dir_path, page_file)

            with open(page_file_path, "r", encoding="utf-8") as f:
                page_text = f.read().replace("\n", "")

            out_f.write(
                f"################## Text Page {page_index + 1} ##################\n"
            )

	prompt = "Please extract the data related to chemicals from the given text and return it in the specified JSON format.\
	- Chemical name: The specific name of the chemical. \
	- Information type: It should be one of the following: "production volume", "production capacity", "production value", "market size", "consumption volume", "import volume", "import value", "export volume", or "export value". 
	- Numerical value: specific value.\
	- Unit: the unit corresponding to the numerical value.\
	- Year: the year corresponding to the numerical value.\
	- Month: the month corresponding to the numerical value.\
	- Location: the specific location corresponding to the numerical value.\
	The output format should be as follows: 
	{'Chemical name': 'XXX' or 'No information', 'Information type': 'XXX' or 'No information', 'Numerical value': 'XXX' or 'No information', 'Unit': 'XXX' or 'No information', 'Year': 'XXX' or 'No information', 'Month': 'XXX' or 'No information', 'Location': 'XXX' or 'No information'}, critical notes: do not output any additional content."+page_text
            
            messages = [
                {
                    "role": "system",
                    "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            chat_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            model_inputs = tokenizer(
                [chat_text], return_tensors="pt"
            ).to(model.device)

            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=8192
            )

            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(
                    model_inputs.input_ids, generated_ids
                )
            ]

            response = tokenizer.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]

            out_f.write(response + "\n")
            print(response)
            print()
